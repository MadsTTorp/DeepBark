{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to run RAG setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import getpass\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "import bs4 \n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, Annotated, TypedDict\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import requests\n",
    "import faiss \n",
    "import numpy as np\n",
    "# from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single document extract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://petguide.dk/hundefoder-maerker/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"entry-content single-page\", \"entry-title\", \"entry-meta uppercase is-xsmall\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the text splitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "\n",
    "total_documents = len(docs)\n",
    "third = total_documents // 3\n",
    "\n",
    "# Split the documents into chunks\n",
    "all_splits = []\n",
    "for doc in docs:\n",
    "    splits = text_splitter.split_documents([doc])\n",
    "    num_splits = len(splits)\n",
    "    third = num_splits // 3\n",
    "    \n",
    "    for i, split in enumerate(splits):\n",
    "        split.metadata[\"source\"] = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        if i < third:\n",
    "            split.metadata[\"section\"] = \"beginning\"\n",
    "        elif i < 2 * third:\n",
    "            split.metadata[\"section\"] = \"middle\"\n",
    "        else:\n",
    "            split.metadata[\"section\"] = \"end\"\n",
    "    all_splits.extend(splits)\n",
    "    \n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all sub-documents to the vector store\n",
    "# document_ids = vector_store.add_documents(documents=all_splits)\n",
    "import numpy as np\n",
    "import faiss\n",
    "# Create embeddings for each chunk\n",
    "embeddings_list = [embeddings.embed_query(doc.page_content) for doc in all_splits]\n",
    "embeddings_array = np.array(embeddings_list)\n",
    "\n",
    "# Create a FAISS index\n",
    "dimension = embeddings_array.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple documents extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_links(main_url: str) -> List[str]:\n",
    "    response = requests.get(main_url)\n",
    "    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    article_links = set(\n",
    "        [\n",
    "            a['href'] \n",
    "            for a in soup.find_all('a', class_='plain', href=True) \n",
    "            if 'https://petguide.dk' in a['href'] and 'kat' not in a['href']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return list(article_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and chunk documents\n",
    "def load_and_chunk_documents(web_paths: List[str]):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=web_paths,\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(\n",
    "                class_=(\"entry-content single-page\", \"entry-title\", \"entry-meta uppercase is-xsmall\")\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Initiate the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # chunk size (characters)\n",
    "        chunk_overlap=200,  # chunk overlap (characters)\n",
    "        add_start_index=True,  # track index in original document\n",
    "    )\n",
    "    # Split the documents into chunks\n",
    "    all_splits = text_splitter.split_documents(docs)\n",
    "    print(f\"Split documents into {len(all_splits)} sub-documents.\")\n",
    "    \n",
    "    # Create embeddings for each chunk\n",
    "    embeddings_list = [embeddings.embed_query(doc.page_content) for doc in all_splits]\n",
    "    embeddings_array = np.array(embeddings_list)\n",
    "    \n",
    "    # Create a FAISS index\n",
    "    dimension = embeddings_array.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_array)\n",
    "  \n",
    "    print(f\"Added {len(all_splits)} documents to the vector store.\")\n",
    "    return all_splits, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting article links from https://petguide.dk/bloggen/...\n",
      "Found 172 article links...\n",
      "Split documents into 28 sub-documents.\n",
      "Added 28 documents to the vector store.\n"
     ]
    }
   ],
   "source": [
    "main_url = \"https://petguide.dk/bloggen/\"\n",
    "print(f'Getting article links from {main_url}...')\n",
    "article_links = get_article_links(main_url)\n",
    "print(f'Found {len(article_links)} article links...')\n",
    "all_splits, index = load_and_chunk_documents(article_links[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect prompt (template) from the hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "prompt_template = \"\"\"Brug følgende stykker kontekst til at besvare spørgsmålet i slutningen. \n",
    "Hvis du ikke kender svaret, så sig bare, at du ikke ved det, og prøv ikke at opdigte et svar.\n",
    "Svar med maksimalt tre sætninger og hold svaret så kortfattet men præcist som muligt.\n",
    "Vær høflig i dit svar.\n",
    "\n",
    "{context} \n",
    "\n",
    "Spørgsmål: {question} \n",
    "\n",
    "Hjælpsomt svar:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class AnswerWithSources(TypedDict):\n",
    "    \"\"\"An answer to the question, with sources.\"\"\"\n",
    "    answer: str\n",
    "    sources: Annotated[\n",
    "        List[str],\n",
    "        ...,\n",
    "        \"List of sources (author + year) used to answer the question\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: AnswerWithSources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    # Create embeddings for the query\n",
    "    query_embedding = embeddings.embed_query(state[\"question\"])\n",
    "    query_embedding = np.array([query_embedding])\n",
    "    \n",
    "    # Perform similarity search\n",
    "    distances, indices = index.search(query_embedding, k=5)\n",
    "    retrieved_docs = [all_splits[i] for i in indices[0]]\n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents for the question: {state['question']}\")\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = custom_rag_prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    structured_llm = llm.with_structured_output(AnswerWithSources)\n",
    "    response = structured_llm.invoke(messages)\n",
    "    # Extract unique URLs from the context\n",
    "    unique_urls = list({doc.metadata['source'] for doc in state[\"context\"]})\n",
    "    \n",
    "    # Update the response with the unique URLs\n",
    "    response['sources'] = unique_urls\n",
    "    \n",
    "    state['answer'] = response\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://petguide.dk/staffordshire-bull-terrier/'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[0].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\"question\": \"Hvilket fodermærke er rig på protein, fedt, er kornfrit og produceres i Canada?\"})\n",
    "\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer: {result[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['context']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "woofwisdom-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
